{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, there are two possible ways to speed up NMF with randomization. \n",
    "\n",
    "One way: \n",
    "* basically follow the algorithm in rNMF up to we get $A \\approx Q B$. Then we solve NMF_KL for $B$. \n",
    "\n",
    "* Difficulty: the most obvious one is we need $B$ to be nonnegative. So we need an algorithm to efficiently optimize (given $A \\geq 0 $)\n",
    "\n",
    "$$min \\ |A - Q B|_F, s.t. Q \\text{ columns orthogonal}, B \\geq 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way:\n",
    "* First get a low-rank approximation of data $A$\n",
    "* Then try to exploit the low rank structure and avoid all $O(np)$ computation. \n",
    "\n",
    "* Difficulty: Hard to avoid $O(np)$. \n",
    "    * If we use gradient descent methods:\n",
    "    \n",
    "    For \n",
    "    $$\\min _ { \\mathbf { h } } C ( \\mathbf { h } ) = D ( \\mathbf { a } | \\mathbf { W h } ) \\text { subject to } \\mathbf { h } \\geq 0$$\n",
    "    \n",
    "    we have $$\\nabla _ { \\mathbf { h } } C ( \\mathbf { h } ) = \\mathbf { W } ^ { T } [ ( \\mathbf { W } \\mathbf { h } ) ^ {( \\beta - 2 )} ( \\mathbf { W } \\mathbf { h } - \\mathbf { a } ) ]$$\n",
    "    \n",
    "    Since for KL divergence $\\beta = 1$, we need to compute $Wh$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
